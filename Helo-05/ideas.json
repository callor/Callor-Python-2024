[
    {
        "Name": "Address_security_issues_in_Ethereum_smart_contracts",
        "Title": "이더리움 스마트 계약의 보안 문제를 해결하기 위한 프레임워크의 설계 및 구현",
        "Experiment": "이 논문은 블록체인 기술을 기반으로 하는 분산형 애플리케이션 플랫폼인 이더리움의 핵심 기능인 스마트 계약에 관한 것입니다. 스마트 계약은 자동화된 계약 실행을 가능하게 하며 다양한 산업 분야에서 활용되고 있습니다. 하지만 스마트 계약의 보안 취약성으로 인해 다양한 보안 문제가 발생하고 있습니다. 이 논문에서는 이더리움 스마트 계약의 보안 문제를 분석하고 이를 해결하기 위한 솔루션을 제안합니다.",
        "Interestingness": 5,
        "Feasibility": 3,
        "Novelty": 3,
        "Novel": true
    },
    {
        "Name": "Implementing_a_framework_for_analyzing_smart_contract_security",
        "Title": "스마트 계약 보안 취약점 분석 및 보안 자동화를 위한 프레임워크 구현",
        "Experiment": "본 논문에서는 블록체인 기술을 기반으로 한 분산형 애플리케이션 플랫폼인 이더리움의 핵심 기능인 스마트 계약의 보안 취약성을 분석하고, 자동화된 취약성 탐지를 수행하여 대응 전략을 수립하는 프레임워크에 대한 설계 및 구현 솔루션을 제안한다.",
        "Interestingness": 4,
        "Feasibility": 4,
        "Novelty": 4,
        "Novel": true
    },
    {
        "Name": "multi_task_learning_char_level_models",
        "Title": "멀티태스크 학습을 통한 문자 수준 언어 모델의 성능 향상",
        "Experiment": "이 연구에서는 다양한 텍스트 데이터세트를 사용하여 다중 작업 학습을 적용한 문자 수준 언어 모델을 훈련합니다. 각 데이터세트는 고유한 도메인을 나타내며, 모델이 여러 도메인에서 동시에 학습하도록 합니다. 다중 작업 학습을 위해 모델의 손실 함수를 각 도메인에 맞게 개별적으로 계산하고, 이를 합산하여 최종 손실을 구합니다. 이를 통해 모델의 일반화 능력을 테스트하고, 여러 도메인에서의 성능을 평가합니다. 모델 아키텍처는 주어진 코드 베이스를 바탕으로 하며, 다중 작업 학습을 지원하도록 수정합니다. 결과는 각각의 도메인에서의 성능 지표와 모델의 일반화 능력을 평가하여 제시합니다.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7
    },
    {
        "Name": "efficient_transformer_training",
        "Title": "효율적인 트랜스포머 모델 훈련을 위한 경량화 기법의 적용 및 성능 평가",
        "Experiment": "이 연구에서는 기존 GPT 모델을 경량화하여 훈련 속도를 높이고 자원 사용을 줄이는 방법을 탐구합니다. 구체적으로는 모델의 파라미터 수를 줄이기 위해 프루닝(pruning) 기법과 지식 증류(knowledge distillation)를 적용합니다. 프루닝은 중요도가 낮은 뉴런을 제거함으로써 모델을 경량화하고, 지식 증류는 큰 모델에서 작은 모델로 지식을 전이하는 방법입니다. 실험은 주어진 소규모 데이터세트를 사용하여 경량화된 모델과 원래 모델의 성능을 비교 평가하며, 훈련 속도, 메모리 사용량, 그리고 예측 정확도를 주요 지표로 삼습니다.",
        "Interestingness": 9,
        "Feasibility": 8,
        "Novelty": 7
    },
    {
        "Name": "quantized_transformer_optimization",
        "Title": "양자화와 효율적인 트랜스포머 변형을 통한 모델 최적화 및 성능 평가",
        "Experiment": "이 연구에서는 기존 GPT 모델에 양자화 기법과 효율적인 트랜스포머 변형을 적용하여 모델의 훈련 및 추론 속도를 향상시키고 자원 사용을 최적화합니다. 양자화는 모델의 가중치와 활성화 값을 저해상도로 표현함으로써 메모리 사용량과 계산 복잡도를 줄이는 방법입니다. 효율적인 트랜스포머 변형으로는 Reformer와 Linformer를 사용하여, 자원 사용을 줄이면서도 높은 성능을 유지하는 방법을 탐구합니다. 실험은 주어진 소규모 데이터세트를 사용하여 양자화된 모델과 원래 모델, 그리고 효율적인 트랜스포머 변형을 적용한 모델의 성능을 비교 평가하며, 훈련 속도, 메모리 사용량, 그리고 예측 정확도를 주요 지표로 삼습니다.",
        "Interestingness": 9,
        "Feasibility": 8,
        "Novelty": 8
    }
]